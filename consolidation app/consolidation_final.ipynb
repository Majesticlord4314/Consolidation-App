{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inventory Consolidation Analysis\n",
    "\n",
    "This notebook performs inventory rebalancing analysis using sales and stock data to generate movement recommendations between stores and warehouses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shivansh Pal\\AppData\\Local\\Temp\\ipykernel_11064\\907951284.py:7: DtypeWarning: Columns (51,63,83,143,158,171,172,180,181,203,233) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_sales = pd.read_csv(r\"C:\\Users\\Shivansh Pal\\Desktop\\Planogramm\\consolidation dataset\\sales.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales data loaded: (169581, 237)\n",
      "SOH data loaded: (74899, 111)\n",
      "Format template loaded: (5, 8)\n",
      "\n",
      "Sales data columns: ['RegionName', 'StoreName', 'AlternateStoreCode', 'StoreGSTNumber', 'BillDate', 'BillTime', 'BillNumber', 'CustomerCode', 'CustomerName', 'CustomerGSTNumber', 'CustomerMobile', 'Quantity', 'FreeQty', 'FreeMRPValue', 'TaxDescription', 'BaseValue', 'OtherTax', 'TNGSTAmount', 'CST', 'SGST', 'CGST', 'IGST', 'UTGST', 'Tax', 'IsTaxInclusive', 'TaxTransactionNumber', 'HSNCode', 'Amount', 'ProductName', 'LanguageProductName', 'BrandName', 'SupplierRefCode', 'CategoryCode', 'Department', 'SubCategory', 'ProductAttribute2', 'ProductAttribute3', 'ProductAttribute4', 'ProductAttribute5', 'ProductCode', 'EANCode', 'AlternateProductCodes', 'Batch', 'MRP', 'ProductLevelDisc%', 'ProductLevelDiscAmount', 'BillLevelDiscAmount', 'BillLevelDisc%', 'TotalDiscAmount', 'TotalDisc%', 'OtherDiscountPercentage', 'ExpiryDate', 'SalesmanCode', 'SalesmanName', 'Classification', 'Cenvat', 'Educess', 'Sheducess', 'ExciseDutyCode', 'Abatement', 'Reason', 'MRPTotal', 'RRPTotal', 'UserName', 'SerialNumberUpdatedBy', 'SerialNumberUpdatedOn', 'UnitDescription', 'OtherDiscount', 'ServiceTaxAmt', 'STCessAmount', 'STEduCessAmount', 'Color', 'Gender', 'Size', 'BusinessSegmentName', 'ArticleNumber', 'DivisionName', 'MarketingDivisionName', 'QuarterSeasonName', 'PricePointName', 'ProductCategoryName', 'SportsName', 'PatientName', 'Attribute1', 'Attribute2', 'Attribute3', 'Attribute4', 'Attribute5', 'Class', 'SubClass', 'Surcharge', 'CessAmt', 'UnitMRP', 'ToonLabel', 'BillSeries', 'PivotProductCode', 'DiscountBudgetName', 'SupplierName', 'ManufacturerCode', 'MarketerCode', 'Rate', 'ActualRate', 'OrderDate', 'OrderNumber', 'InsApprovalCode', 'CoPayerPercentage', 'CoPayerAmount', 'CAAddress1', 'CAAddress2', 'CAAddress3', 'CACityName', 'CAStateName', 'OfferApprovedBy', 'RateModifiedBy', 'TerminalNumber', 'ExchangeBillDate', 'ExchangeBillNumber', 'Discount Type', 'UOMDesc', 'UOMQty', 'ManufacturedDate', 'AdditionalCESS', 'ZoneCode', 'ZoneName', 'BNumber', 'ProductDescription', 'CompanyEmployeeID', 'StateName', 'DiscType', 'RefPromoCode', 'PromotionType', 'SchemeCode', 'BillLevelPromoCode', 'PromotionName', 'SchemeGroupName', 'StoreCode', 'StoreClusterName', 'CityName', 'Address1', 'Address2', 'Address3', 'PinCode', 'Email', 'Remarks', 'ProductGroup', 'AlternateGroupCode', 'TransactionAttributes', 'DeliveryType', 'IRN', 'FreeValue', 'InsuranceCompanyCode', 'InsuranceCompanyName', 'WSDN', 'ManufacturerName', 'FixedCESS', 'AcknowledgeDate', 'AcknowledgeNumber', 'CustomerGroupName', 'ChannelName', 'TCS', 'AlternateCustomerCode', 'ScheduleTypeCode', 'ScheduleTypeDescription', 'DoctorName', 'Combination', 'SubLocation', 'SubLocationName', 'ChapterNumber', 'StockSourceStoreName', 'StockSourceStore', 'ReferenceDocument', 'OrderReferenceNo', 'OrderReferenceDate', 'CustomerTierName', 'CustomerTypeName', 'DivisionCode', 'ProductGroupCode', 'AlternateProductGroupCode', 'AlternateDivisionCode', 'Season', 'RefChannelName', 'RefChannelOrderNo', 'InvoiceNumber', 'ShipmentNumber', 'IsBudgetedDiscount', 'PivotStoreCode', 'New customer Type', 'ProductImage', 'AlphaBatchID', 'StoreAbbrevation', 'OrderReference', 'BillRemarks', 'AlternateCode2', 'PurchaseUnit', 'BulkDescription', 'ItemCost', 'CostOfSale', 'PackageInformation', 'ProviderTaxCode', 'TaxPercentage', 'ProductFullName', 'BatchID', 'BatchCode', 'TINNumber', 'GrossWithDiscount', 'SupplierState', 'SupplierGSTNumber', 'SupplierPinCode', 'TransactionDate', 'BatchSupplier', 'SubscriptionID', 'StoreBrand', 'StoreType', 'ReferenceDocNo', 'Country', 'WarrantyPaymentStatus', 'BatchInvoiceDate', 'BatchInvoiceNumber', 'GLN', 'GTIN', 'BatchSerialNo', 'BA Address1', 'BA Address2', 'BA Address3', 'BA Pincode', 'BA City', 'BA State', 'ScannedCode', 'DiscountName', 'BuyBackProductCode', 'BuyBackAlternateProductCode', 'BuyBackProductName', 'BuyBackRemark', 'LastDateOfWarranty', 'EmployeeDepartment', 'DeliveryState', 'Month']\n",
      "SOH data columns: ['DATE', 'RegionCode', 'RegionName', 'StoreCode', 'StoreName', 'AlternateStoreCode', 'ProductCode', 'ProductName', 'InvoiceNumber', 'InvoiceDate', 'GRNNumber', 'GRNDate', 'Department', 'Class', 'SubClass', 'AgeGroup', 'Gender', 'Color', 'Size', 'GarmentType', 'Season', 'EanCode', 'ToonLabel', 'IndentProductGroup', 'SupplierRefCode', 'AlternateProductCodes', 'BatchDescription', 'PurchaseUnit', 'Stock', 'Stock UOM', 'ExpiryDate', 'ShelfLifeInDays', 'DaysTranspired', 'DaysLeftForExpiry', 'IsLocked', 'Reason', 'ConsumptionType', 'MRP', 'MRPValue', 'SalePrice', 'PurchasePrice', 'SalesTax', 'PurchaseTax', 'ArticleNumber', 'BusinessSegmentName', 'DivisionName', 'MarketingDivisionName', 'QuarterSeasonName', 'PricePointName', 'ProductCategoryName', 'SportsName', 'BatchCode', 'Report Stock', 'UOM', 'UnitMRP', 'ManufacturerCode', 'MarketerCode', 'SupplierCode', 'SupplierName', 'DiscountedStock', 'NonDiscountedStock', 'DiscountAmount', 'DiscountPercentage', 'RRP', 'SchemeDescription', 'StoreBrand', 'ZoneCode', 'ZoneName', 'ProductDescription', 'Brand', 'ProductGroup', 'StateName', 'AlternateClusterCode', 'StoreClusterName', 'AlternateGroupCode', 'ReservedStock', 'ActualStock', 'ManufacturedDate', 'LocationName', 'TransferInNumber', 'TransferInDate', 'LastGRNNumber', 'LastReceivedDate', 'ItemCost', 'ScheduleTypeCode', 'ScheduleTypeDescription', 'Combination', 'SubLocation', 'SubLocationName', 'StockLocked', 'BatchID', 'AlphaBatchID', 'StockTransferFrom', 'AdditionalCost', 'UnitDescription', 'UDCode', 'MasterUnitDescription', 'LastSaleDate', 'FirstReceivedDate', 'FirstSoldDate', 'CreatedDate', 'ModifiedDate', 'LastAccessedDate', 'InWardoffer', 'Attribute1', 'PivotStoreCode', 'StoreType', 'GLN', 'GTIN', 'BatchSerialNo', 'PivotProductCode']\n",
      "Format template columns: ['Part No', 'From Store', 'To Store', 'From Store Current SOH', 'To store Current SOH', 'From STORE sales', 'TO store Sales', 'Sugested Qty transfer']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shivansh Pal\\AppData\\Local\\Temp\\ipykernel_11064\\907951284.py:8: DtypeWarning: Columns (25,47,91,97,99,104) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_soh = pd.read_csv(r\"C:\\Users\\Shivansh Pal\\Desktop\\Planogramm\\consolidation dataset\\Stock Report Batchwise (59).csv\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the datasets\n",
    "print(\"Loading datasets...\")\n",
    "\n",
    "df_sales = pd.read_csv(r\"C:\\Users\\Shivansh Pal\\Desktop\\Planogramm\\consolidation dataset\\sales.csv\")\n",
    "df_soh = pd.read_csv(r\"C:\\Users\\Shivansh Pal\\Desktop\\Planogramm\\consolidation dataset\\Stock Report Batchwise (59).csv\")\n",
    "df_format = pd.read_csv(r\"C:\\Users\\Shivansh Pal\\Desktop\\Planogramm\\consolidation dataset\\Consolidation format.csv\")\n",
    "\n",
    "# Standardize column names: strip whitespace\n",
    "df_sales.columns = df_sales.columns.str.strip()\n",
    "df_soh.columns = df_soh.columns.str.strip()\n",
    "df_format.columns = df_format.columns.str.strip()\n",
    "\n",
    "print(f\"Sales data loaded: {df_sales.shape}\")\n",
    "print(f\"SOH data loaded: {df_soh.shape}\")\n",
    "print(f\"Format template loaded: {df_format.shape}\")\n",
    "\n",
    "print(\"\\nSales data columns:\", df_sales.columns.tolist())\n",
    "print(\"SOH data columns:\", df_soh.columns.tolist())\n",
    "print(\"Format template columns:\", df_format.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All required columns found in all files.\n"
     ]
    }
   ],
   "source": [
    "# Check for required columns and raise error if missing\n",
    "\n",
    "required_sales_cols = ['StoreName', 'ProductCode', 'ProductName', 'Quantity']\n",
    "required_soh_cols = ['StoreName', 'ProductCode', 'ProductName', 'ActualStock']\n",
    "required_format_cols = ['Part No']  # Only require 'Part No' in the format file\n",
    "\n",
    "for col in required_sales_cols:\n",
    "    if col not in df_sales.columns:\n",
    "        raise ValueError(f\"Missing column in sales.csv: {col}\")\n",
    "for col in required_soh_cols:\n",
    "    if col not in df_soh.columns:\n",
    "        raise ValueError(f\"Missing column in Stock Report: {col}\")\n",
    "for col in required_format_cols:\n",
    "    if col not in df_format.columns:\n",
    "        raise ValueError(f\"Missing column in format file: {col}\")\n",
    "\n",
    "print(\"All required columns found in all files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for warehouses in datasets...\n",
      "Warehouses in sales data: 0\n",
      "Warehouses in SOH data: 7\n",
      "\n",
      "Sample warehouses from SOH data:\n",
      "  1. Warehouse Bose2- R T Nagar Bengaluru\n",
      "  2. Warehouse Bose 2- Lakshmipuram Thiruvanmiyur Chennai\n",
      "  3. Warehouse Bose 2- Punjagutta Hyderabad\n",
      "  4. Warehouse Imagine 3- Jayanagar Bengaluru\n",
      "  5. Warehouse Imagine 3- Lakshmipuram Thiruvanmiyur Chennai\n",
      "\n",
      "Total unique stores in sales: 100\n",
      "Total unique stores in SOH: 110\n"
     ]
    }
   ],
   "source": [
    "# Check for warehouses in both datasets\n",
    "print(\"Checking for warehouses in datasets...\")\n",
    "\n",
    "sales_stores = df_sales['StoreName'].unique()\n",
    "soh_stores = df_soh['StoreName'].unique()\n",
    "\n",
    "sales_warehouses = [store for store in sales_stores if 'warehouse' in str(store).lower()]\n",
    "soh_warehouses = [store for store in soh_stores if 'warehouse' in str(store).lower()]\n",
    "\n",
    "print(f\"Warehouses in sales data: {len(sales_warehouses)}\")\n",
    "print(f\"Warehouses in SOH data: {len(soh_warehouses)}\")\n",
    "\n",
    "if soh_warehouses:\n",
    "    print(\"\\nSample warehouses from SOH data:\")\n",
    "    for i, wh in enumerate(soh_warehouses[:5]):\n",
    "        print(f\"  {i+1}. {wh}\")\n",
    "\n",
    "print(f\"\\nTotal unique stores in sales: {len(sales_stores)}\")\n",
    "print(f\"Total unique stores in SOH: {len(soh_stores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and preparing data...\n",
      "  Removed 88243 Apple products\n",
      "Sales aggregated: 14980 store-product combinations\n",
      "SOH aggregated: 41893 store-product combinations\n",
      "Data types converted to numeric\n"
     ]
    }
   ],
   "source": [
    "# Data cleaning and preparation\n",
    "print(\"Cleaning and preparing data...\")\n",
    "\n",
    "# Clean sales data - remove Apple products\n",
    "def clean_data(df):\n",
    "    brand_col = 'BrandName' if 'BrandName' in df.columns else 'Brand'\n",
    "    if brand_col in df.columns:\n",
    "        initial_count = len(df)\n",
    "        df = df[df[brand_col] != 'Apple']\n",
    "        print(f\"  Removed {initial_count - len(df)} Apple products\")\n",
    "    return df\n",
    "\n",
    "df_sales_clean = clean_data(df_sales.copy())\n",
    "\n",
    "# Prepare sales data - aggregate by store and product\n",
    "sales_agg = df_sales_clean.groupby(['StoreName', 'ProductDescription'], as_index=False).agg({\n",
    "    'Quantity': 'sum',\n",
    "    'Amount': 'sum'\n",
    "}).rename(columns={\n",
    "    'StoreName': 'Store',\n",
    "    'ProductDescription': 'Product', \n",
    "    'Quantity': 'Total_Quantity_Sold',\n",
    "    'Amount': 'Sales'\n",
    "})\n",
    "\n",
    "# Prepare SOH data - aggregate by store and product\n",
    "soh_agg = df_soh.groupby(['StoreName', 'ProductDescription'], as_index=False).agg({\n",
    "    'ActualStock': 'sum',\n",
    "    'ProductName': 'first'  # Keep product name for mapping\n",
    "}).rename(columns={\n",
    "    'StoreName': 'Store',\n",
    "    'ProductDescription': 'Product',\n",
    "    'ActualStock': 'Stock'\n",
    "})\n",
    "\n",
    "print(f\"Sales aggregated: {sales_agg.shape[0]} store-product combinations\")\n",
    "print(f\"SOH aggregated: {soh_agg.shape[0]} store-product combinations\")\n",
    "\n",
    "# Convert numeric columns\n",
    "sales_agg['Total_Quantity_Sold'] = pd.to_numeric(sales_agg['Total_Quantity_Sold'], errors='coerce').fillna(0)\n",
    "sales_agg['Sales'] = pd.to_numeric(sales_agg['Sales'], errors='coerce').fillna(0)\n",
    "soh_agg['Stock'] = pd.to_numeric(soh_agg['Stock'], errors='coerce').fillna(0)\n",
    "\n",
    "print(\"Data types converted to numeric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging sales and stock data...\n",
      "Merged data shape: (48141, 6)\n",
      "Total products: 4955\n",
      "Total stores: 112\n",
      "\n",
      "Sample merged data:\n",
      "                          Store      Product  Total_Quantity_Sold     Sales  \\\n",
      "0  Bose- Ambience Mall Gurugram        24643                  5.0  235907.5   \n",
      "1  Bose- Ambience Mall Gurugram        24644                  0.0       0.0   \n",
      "2  Bose- Ambience Mall Gurugram  722139-0010                  3.0   25200.0   \n",
      "3  Bose- Ambience Mall Gurugram  722139-0020                  0.0       0.0   \n",
      "4  Bose- Ambience Mall Gurugram  722140-0020                  0.0       0.0   \n",
      "\n",
      "   Stock                          ProductName  \n",
      "0    1.0  251 ENVIRONMENTAL BLK WITH BLK BRKT  \n",
      "1    1.0  251 ENVIRONMENTAL WHT WITH WHT BRKT  \n",
      "2    0.0                                  NaN  \n",
      "3    1.0   UFS 20 II UNIVERSAL FLOORSTAND WHT  \n",
      "4    1.0  UTS-20 II UNIVERSAL TABLE STAND WHT  \n"
     ]
    }
   ],
   "source": [
    "# Merge sales and stock data\n",
    "print(\"Merging sales and stock data...\")\n",
    "\n",
    "# Merge on store and product\n",
    "df_merged = pd.merge(sales_agg, soh_agg, on=['Store', 'Product'], how='outer')\n",
    "df_merged['Total_Quantity_Sold'] = df_merged['Total_Quantity_Sold'].fillna(0)\n",
    "df_merged['Sales'] = df_merged['Sales'].fillna(0)\n",
    "df_merged['Stock'] = df_merged['Stock'].fillna(0)\n",
    "\n",
    "print(f\"Merged data shape: {df_merged.shape}\")\n",
    "print(f\"Total products: {df_merged['Product'].nunique()}\")\n",
    "print(f\"Total stores: {df_merged['Store'].nunique()}\")\n",
    "\n",
    "# Display sample merged data\n",
    "print(\"\\nSample merged data:\")\n",
    "print(df_merged.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating demand forecasts...\n",
      "Average monthly sale rate: 0.49\n",
      "Average forecast demand: 1.47\n",
      "Total current stock: 219263\n",
      "Total forecasted demand: 70610\n"
     ]
    }
   ],
   "source": [
    "# Demand forecasting\n",
    "print(\"Calculating demand forecasts...\")\n",
    "\n",
    "# Calculate monthly sale rate (assuming 3-month data)\n",
    "n_months = 3\n",
    "df_merged['Monthly_Sale_Rate'] = df_merged['Total_Quantity_Sold'] / n_months\n",
    "df_merged['Forecast_Demand'] = df_merged['Monthly_Sale_Rate'] * n_months\n",
    "\n",
    "# Cap unrealistic forecasts\n",
    "df_merged['Monthly_Sale_Rate'] = df_merged['Monthly_Sale_Rate'].clip(upper=100)\n",
    "df_merged['Forecast_Demand'] = df_merged['Monthly_Sale_Rate'] * n_months\n",
    "\n",
    "print(f\"Average monthly sale rate: {df_merged['Monthly_Sale_Rate'].mean():.2f}\")\n",
    "print(f\"Average forecast demand: {df_merged['Forecast_Demand'].mean():.2f}\")\n",
    "print(f\"Total current stock: {df_merged['Stock'].sum():.0f}\")\n",
    "print(f\"Total forecasted demand: {df_merged['Forecast_Demand'].sum():.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying stores and warehouses...\n",
      "Product name mappings: 4528\n",
      "Warehouses identified: 7\n",
      "Retail stores identified: 105\n",
      "Sample warehouses:\n",
      "  - Warehouse Bose2- R T Nagar Bengaluru\n",
      "  - Warehouse Bose 2- Punjagutta Hyderabad\n",
      "  - Warehouse Imagine 3- Jayanagar Bengaluru\n",
      "  - Warehouse Bose 2- Lakshmipuram Thiruvanmiyur Chennai\n",
      "  - Warehouse- Tech 4 Jayanagar Bengaluru\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "# Classify stores and warehouses\n",
    "print(\"Classifying stores and warehouses...\")\n",
    "\n",
    "# Create lookup dictionaries\n",
    "stock_lookup = dict(zip(zip(df_merged['Store'], df_merged['Product']), df_merged['Stock']))\n",
    "rate_lookup = dict(zip(zip(df_merged['Store'], df_merged['Product']), df_merged['Monthly_Sale_Rate']))\n",
    "sales_lookup = dict(zip(zip(df_merged['Store'], df_merged['Product']), df_merged['Total_Quantity_Sold']))\n",
    "\n",
    "# Product name mapping\n",
    "product_name_map = df_merged.dropna(subset=['ProductName']).drop_duplicates(subset=['Product'])[['Product', 'ProductName']].set_index('Product')['ProductName'].to_dict()\n",
    "print(f\"Product name mappings: {len(product_name_map)}\")\n",
    "\n",
    "# Classify warehouses vs retail stores\n",
    "warehouse_by_prod = defaultdict(list)\n",
    "stores_by_prod = defaultdict(list)\n",
    "\n",
    "for store, prod in stock_lookup:\n",
    "    store_lower = str(store).lower()\n",
    "    is_warehouse = (\n",
    "        'warehouse' in store_lower or \n",
    "        'wh-' in store_lower or \n",
    "        'depot' in store_lower or\n",
    "        'distribution' in store_lower or\n",
    "        'central' in store_lower or\n",
    "        store_lower.startswith('wh ')\n",
    "    )\n",
    "    \n",
    "    if is_warehouse:\n",
    "        warehouse_by_prod[prod].append(store)\n",
    "    else:\n",
    "        stores_by_prod[prod].append(store)\n",
    "\n",
    "warehouse_count = len(set([store for stores in warehouse_by_prod.values() for store in stores]))\n",
    "retail_count = len(set([store for stores in stores_by_prod.values() for store in stores]))\n",
    "\n",
    "print(f\"Warehouses identified: {warehouse_count}\")\n",
    "print(f\"Retail stores identified: {retail_count}\")\n",
    "\n",
    "if warehouse_count > 0:\n",
    "    sample_warehouses = list(set([store for stores in warehouse_by_prod.values() for store in stores]))[:5]\n",
    "    print(\"Sample warehouses:\")\n",
    "    for wh in sample_warehouses:\n",
    "        print(f\"  - {wh}\")\n",
    "else:\n",
    "    print(\"\\nWarning: No warehouses identified by naming pattern.\")\n",
    "    print(\"Using stores with highest stock as warehouses...\")\n",
    "    \n",
    "    # Use top stores by average stock as warehouses\n",
    "    store_avg_stock = df_merged.groupby('Store')['Stock'].mean().sort_values(ascending=False)\n",
    "    warehouse_threshold = max(5, len(store_avg_stock) // 20)  # Top 5% or at least 5 stores\n",
    "    warehouse_stores = store_avg_stock.head(warehouse_threshold).index.tolist()\n",
    "    \n",
    "    print(f\"Using top {len(warehouse_stores)} stores by stock as warehouses\")\n",
    "    \n",
    "    # Reassign based on stock levels\n",
    "    warehouse_by_prod = defaultdict(list)\n",
    "    stores_by_prod = defaultdict(list)\n",
    "    \n",
    "    for store, prod in stock_lookup:\n",
    "        if store in warehouse_stores:\n",
    "            warehouse_by_prod[prod].append(store)\n",
    "        else:\n",
    "            stores_by_prod[prod].append(store)\n",
    "    \n",
    "    warehouse_count = len(set([store for stores in warehouse_by_prod.values() for store in stores]))\n",
    "    retail_count = len(set([store for stores in stores_by_prod.values() for store in stores]))\n",
    "    \n",
    "    print(f\"Reassigned - Warehouses: {warehouse_count}, Retail stores: {retail_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating product movements...\n",
      "Total movements calculated: 7979\n",
      "Total quantity to be moved: 23294\n"
     ]
    }
   ],
   "source": [
    "# Calculate product movements\n",
    "print(\"Calculating product movements...\")\n",
    "\n",
    "movements = []\n",
    "processed_combinations = set()\n",
    "\n",
    "# Copy stock lookup for modification during movement calculation\n",
    "available_stock = stock_lookup.copy()\n",
    "\n",
    "for _, row in df_merged.iterrows():\n",
    "    dest = row['Store']\n",
    "    prod = row['Product']\n",
    "    \n",
    "    # Skip if already processed\n",
    "    if (dest, prod) in processed_combinations:\n",
    "        continue\n",
    "    processed_combinations.add((dest, prod))\n",
    "    \n",
    "    current_stock = float(available_stock.get((dest, prod), 0))\n",
    "    forecast_demand = float(row['Forecast_Demand'])\n",
    "    demand_gap = forecast_demand - current_stock\n",
    "    \n",
    "    if demand_gap <= 0:\n",
    "        continue\n",
    "    \n",
    "    remaining_demand = demand_gap\n",
    "    \n",
    "    # First try to fulfill from warehouses\n",
    "    for src in warehouse_by_prod.get(prod, []):\n",
    "        if remaining_demand <= 0 or src == dest:\n",
    "            continue\n",
    "            \n",
    "        available = float(available_stock.get((src, prod), 0))\n",
    "        transfer_qty = min(available, remaining_demand)\n",
    "        \n",
    "        if transfer_qty > 0:\n",
    "            movements.append({\n",
    "                'ProductName': product_name_map.get(prod, prod),\n",
    "                'Product': prod,\n",
    "                'Source': src,\n",
    "                'Destination': dest,\n",
    "                'Quantity': transfer_qty\n",
    "            })\n",
    "            available_stock[(src, prod)] = max(0, available_stock.get((src, prod), 0) - transfer_qty)\n",
    "            remaining_demand -= transfer_qty\n",
    "    \n",
    "    # Then try from other retail stores (prioritize low-velocity stores)\n",
    "    if remaining_demand > 0:\n",
    "        available_stores = [s for s in stores_by_prod.get(prod, []) if s != dest]\n",
    "        # Sort by monthly sale rate (ascending) to prioritize low-velocity stores\n",
    "        sorted_stores = sorted(available_stores, key=lambda s: rate_lookup.get((s, prod), 0))\n",
    "        \n",
    "        for src in sorted_stores:\n",
    "            if remaining_demand <= 0:\n",
    "                break\n",
    "                \n",
    "            available = float(available_stock.get((src, prod), 0))\n",
    "            transfer_qty = min(available, remaining_demand)\n",
    "            \n",
    "            if transfer_qty > 0:\n",
    "                movements.append({\n",
    "                    'ProductName': product_name_map.get(prod, prod),\n",
    "                    'Product': prod,\n",
    "                    'Source': src,\n",
    "                    'Destination': dest,\n",
    "                    'Quantity': transfer_qty\n",
    "                })\n",
    "                available_stock[(src, prod)] = max(0, available_stock.get((src, prod), 0) - transfer_qty)\n",
    "                remaining_demand -= transfer_qty\n",
    "\n",
    "print(f\"Total movements calculated: {len(movements)}\")\n",
    "if movements:\n",
    "    total_qty = sum(m['Quantity'] for m in movements)\n",
    "    print(f\"Total quantity to be moved: {total_qty:.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movement DataFrame created: (7979, 5)\n",
      "Products involved: 1151\n",
      "Source locations: 107\n",
      "Destination locations: 97\n",
      "\n",
      "Movements from warehouses: 1933 (7301 units)\n",
      "Movements from retail stores: 6046 (15993 units)\n",
      "\n",
      "Top 10 largest movements:\n",
      "  TEKNE Spotfree Cleaning Spray... 225 units: Warehouse Imagine 3- Jayanagar Bengaluru → Imagine- Lulu Mall Kochi\n",
      "  TEKNE Spotfree Cleaning Spray... 187 units: Warehouse Imagine 3- Jayanagar Bengaluru → Imagine- Nexus Koramangala Bengaluru\n",
      "  TEKNE Spotfree Cleaning Spray... 121 units: Warehouse Imagine 3- Jayanagar Bengaluru → Imagine- HSR Layout Bengaluru\n",
      "  Jute Bag Large - APR... 100 units: Bose- GIP Noida → Imagine- Express Avenue Chennai\n",
      "  Jute Bag Large - APR... 100 units: Bose- Inorbit Mall Malad Mumbai → Imagine- HSR Layout Bengaluru\n",
      "  Jute Bag Large - APR... 100 units: Bose- Inorbit Mall Vashi Mumbai → Imagine- HSR Layout Bengaluru\n",
      "  Jute Bag Large - APR... 100 units: Bose- Khan Marlet New Delhi → Imagine- Koramangala Bengaluru\n",
      "  Jute Bag Large - APR... 100 units: Bose- MGF Metropolitan Mall Jaipur → Imagine- Lulu Mall Trivandrum\n",
      "  Jute Bag Large - APP... 100 units: Bose- GIP Noida → Imagine- Nexus Koramangala Bengaluru\n",
      "  Jute Bag Large - APR... 100 units: Bose- Phoenix Market City Pune → Imagine- Nexus Vega City Bengaluru\n"
     ]
    }
   ],
   "source": [
    "# Create movement DataFrame\n",
    "if movements:\n",
    "    mov_df = pd.DataFrame(movements)\n",
    "    \n",
    "    print(f\"Movement DataFrame created: {mov_df.shape}\")\n",
    "    print(f\"Products involved: {mov_df['Product'].nunique()}\")\n",
    "    print(f\"Source locations: {mov_df['Source'].nunique()}\")\n",
    "    print(f\"Destination locations: {mov_df['Destination'].nunique()}\")\n",
    "    \n",
    "    # Check warehouse vs retail sources\n",
    "    warehouse_movements = mov_df[mov_df['Source'].str.lower().str.contains('warehouse', na=False)]\n",
    "    retail_movements = mov_df[~mov_df['Source'].str.lower().str.contains('warehouse', na=False)]\n",
    "    \n",
    "    print(f\"\\nMovements from warehouses: {len(warehouse_movements)} ({warehouse_movements['Quantity'].sum():.0f} units)\")\n",
    "    print(f\"Movements from retail stores: {len(retail_movements)} ({retail_movements['Quantity'].sum():.0f} units)\")\n",
    "    \n",
    "    print(\"\\nTop 10 largest movements:\")\n",
    "    top_movements = mov_df.nlargest(10, 'Quantity')[['ProductName', 'Source', 'Destination', 'Quantity']]\n",
    "    for _, row in top_movements.iterrows():\n",
    "        print(f\"  {row['ProductName'][:50]}... {row['Quantity']:.0f} units: {row['Source']} → {row['Destination']}\")\n",
    "else:\n",
    "    print(\"No movements needed - inventory appears well balanced\")\n",
    "    mov_df = pd.DataFrame(columns=['ProductName', 'Product', 'Source', 'Destination', 'Quantity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating consolidation format output...\n",
      "Consolidation format created: (7979, 9)\n",
      "Total movements: 7979\n",
      "Total quantity to transfer: 23294\n",
      "\n",
      "Sample consolidation format output:\n",
      "       Part No                          ProductName  \\\n",
      "0        24643  251 ENVIRONMENTAL BLK WITH BLK BRKT   \n",
      "1        24643  251 ENVIRONMENTAL BLK WITH BLK BRKT   \n",
      "2        24643  251 ENVIRONMENTAL BLK WITH BLK BRKT   \n",
      "3        24643  251 ENVIRONMENTAL BLK WITH BLK BRKT   \n",
      "4  722139-0010   UFS-20 II UNIVERSAL FLOORSTAND BLK   \n",
      "\n",
      "                                     From Store                      To Store  \\\n",
      "0                Bose- Bharatiya City Bengaluru  Bose- Ambience Mall Gurugram   \n",
      "1              Bose- Emporium MG Road Bengaluru  Bose- Ambience Mall Gurugram   \n",
      "2                  Bose- Inorbit Mall Hyderabad  Bose- Ambience Mall Gurugram   \n",
      "3               Bose- Inorbit Mall Malad Mumbai  Bose- Ambience Mall Gurugram   \n",
      "4  Bose- Vintage Boulevard Somajiguda Hyderabad  Bose- Ambience Mall Gurugram   \n",
      "\n",
      "   From Store Current SOH  To store Current SOH  From STORE sales  \\\n",
      "0                     1.0                   1.0               0.0   \n",
      "1                     1.0                   1.0               0.0   \n",
      "2                     1.0                   1.0               0.0   \n",
      "3                     1.0                   1.0               0.0   \n",
      "4                     1.0                   0.0               0.0   \n",
      "\n",
      "   TO store Sales  Sugested Qty transfer  \n",
      "0             5.0                    1.0  \n",
      "1             5.0                    1.0  \n",
      "2             5.0                    1.0  \n",
      "3             5.0                    1.0  \n",
      "4             3.0                    1.0  \n"
     ]
    }
   ],
   "source": [
    "# Format output according to consolidation format\n",
    "print(\"Creating consolidation format output...\")\n",
    "\n",
    "# Required columns: Part No, ProductName, From Store, To Store, From Store Current SOH, To store Current SOH, From STORE sales, TO store Sales, Sugested Qty transfer\n",
    "\n",
    "consolidation_results = []\n",
    "\n",
    "# Use the same source as your movement calculation (movements or mov_df)\n",
    "# If you have a DataFrame: use mov_df.iterrows()\n",
    "# If you have a list of dicts: use for m in movements\n",
    "movement_source = mov_df if 'mov_df' in locals() else movements\n",
    "\n",
    "for row in (movement_source.iterrows() if isinstance(movement_source, pd.DataFrame) else enumerate(movement_source)):\n",
    "    if isinstance(movement_source, pd.DataFrame):\n",
    "        row = row[1]  # get the Series from (idx, Series)\n",
    "    # If using dicts, row is already the dict\n",
    "\n",
    "    part_no = row['Product'] if 'Product' in row else row['Part No']\n",
    "    product_name = row.get('ProductName', part_no)\n",
    "    from_store = row['Source'] if 'Source' in row else row['From Store']\n",
    "    to_store = row['Destination'] if 'Destination' in row else row['To Store']\n",
    "    suggested_qty = row['Quantity'] if 'Quantity' in row else row['Sugested Qty transfer']\n",
    "\n",
    "    # Get current SOH for both stores\n",
    "    from_soh = stock_lookup.get((from_store, part_no), 0)\n",
    "    to_soh = stock_lookup.get((to_store, part_no), 0)\n",
    "\n",
    "    # Get sales for both stores\n",
    "    from_sales = sales_lookup.get((from_store, part_no), 0)\n",
    "    to_sales = sales_lookup.get((to_store, part_no), 0)\n",
    "\n",
    "    consolidation_results.append({\n",
    "        'Part No': part_no,\n",
    "        'ProductName': product_name,\n",
    "        'From Store': from_store,\n",
    "        'To Store': to_store,\n",
    "        'From Store Current SOH': from_soh,\n",
    "        'To store Current SOH': to_soh,\n",
    "        'From STORE sales': from_sales,\n",
    "        'TO store Sales': to_sales,\n",
    "        'Sugested Qty transfer': suggested_qty\n",
    "    })\n",
    "\n",
    "df_format_result = pd.DataFrame(consolidation_results)\n",
    "\n",
    "print(f\"Consolidation format created: {df_format_result.shape}\")\n",
    "\n",
    "if not df_format_result.empty:\n",
    "    print(f\"Total movements: {len(df_format_result)}\")\n",
    "    print(f\"Total quantity to transfer: {df_format_result['Sugested Qty transfer'].sum():.0f}\")\n",
    "\n",
    "    print(\"\\nSample consolidation format output:\")\n",
    "    print(df_format_result.head())\n",
    "else:\n",
    "    print(\"No consolidation movements generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results...\n",
      "Files saved successfully:\n",
      "  - Consolidation format: C:\\Users\\Shivansh Pal\\Desktop\\Planogramm\\consolidation dataset\\consolidation_result.csv\n",
      "  - Detailed movements: C:\\Users\\Shivansh Pal\\Desktop\\Planogramm\\consolidation dataset\\detailed_movements.csv\n",
      "  - Analysis summary: C:\\Users\\Shivansh Pal\\Desktop\\Planogramm\\consolidation dataset\\analysis_summary.csv\n",
      "\n",
      "=== CONSOLIDATION ANALYSIS COMPLETE ===\n",
      "✓ 7979 movements recommended\n",
      "✓ 23294 total units to transfer\n",
      "✓ 1151 products involved\n",
      "✓ 97 destination stores\n"
     ]
    }
   ],
   "source": [
    "# Save results\n",
    "print(\"Saving results...\")\n",
    "\n",
    "# Save consolidation format\n",
    "output_path_consolidation = r'C:\\Users\\Shivansh Pal\\Desktop\\Planogramm\\consolidation dataset\\consolidation_result.csv'\n",
    "df_format_result.to_csv(output_path_consolidation, index=False)\n",
    "\n",
    "# Save detailed movements\n",
    "output_path_movements = r'C:\\Users\\Shivansh Pal\\Desktop\\Planogramm\\consolidation dataset\\detailed_movements.csv'\n",
    "mov_df.to_csv(output_path_movements, index=False)\n",
    "\n",
    "# Save analysis summary\n",
    "summary_data = {\n",
    "    'Total_Products_Processed': [df_merged['Product'].nunique()],\n",
    "    'Total_Stores_Analyzed': [df_merged['Store'].nunique()],\n",
    "    'Warehouses_Identified': [warehouse_count],\n",
    "    'Retail_Stores_Identified': [retail_count],\n",
    "    'Total_Movements_Recommended': [len(df_format_result)],\n",
    "    'Total_Quantity_To_Transfer': [df_format_result['Sugested Qty transfer'].sum() if not df_format_result.empty else 0],\n",
    "    'Total_Current_Stock': [df_merged['Stock'].sum()],\n",
    "    'Total_Forecasted_Demand': [df_merged['Forecast_Demand'].sum()]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_path = r'C:\\Users\\Shivansh Pal\\Desktop\\Planogramm\\consolidation dataset\\analysis_summary.csv'\n",
    "summary_df.to_csv(summary_path, index=False)\n",
    "\n",
    "print(\"Files saved successfully:\")\n",
    "print(f\"  - Consolidation format: {output_path_consolidation}\")\n",
    "print(f\"  - Detailed movements: {output_path_movements}\")\n",
    "print(f\"  - Analysis summary: {summary_path}\")\n",
    "\n",
    "print(\"\\n=== CONSOLIDATION ANALYSIS COMPLETE ===\")\n",
    "if not df_format_result.empty:\n",
    "    print(f\"✓ {len(df_format_result)} movements recommended\")\n",
    "    print(f\"✓ {df_format_result['Sugested Qty transfer'].sum():.0f} total units to transfer\")\n",
    "    print(f\"✓ {df_format_result['Part No'].nunique()} products involved\")\n",
    "    print(f\"✓ {df_format_result['To Store'].nunique()} destination stores\")\n",
    "else:\n",
    "    print(\"✓ No movements needed - inventory is well balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Excel report from consolidation_result.csv...\n",
      "Excel report created successfully: C:\\Users\\Shivansh Pal\\Desktop\\Planogramm\\consolidation dataset\\consolidation_report.xlsx\n",
      "File contains 7979 movements across 9 columns\n",
      "Sheets created: 'Consolidation_Report' and 'Summary'\n"
     ]
    }
   ],
   "source": [
    "# Create Excel report from the perfect CSV file\n",
    "print(\"Creating Excel report from consolidation_result.csv...\")\n",
    "\n",
    "# Read the perfect CSV file\n",
    "csv_path = r'C:\\Users\\Shivansh Pal\\Desktop\\Planogramm\\consolidation dataset\\consolidation_result.csv'\n",
    "excel_path = r'C:\\Users\\Shivansh Pal\\Desktop\\Planogramm\\consolidation dataset\\consolidation_report.xlsx'\n",
    "\n",
    "# Read the CSV\n",
    "df_excel = pd.read_csv(csv_path)\n",
    "\n",
    "# Create Excel writer with formatting\n",
    "with pd.ExcelWriter(excel_path, engine='xlsxwriter') as writer:\n",
    "    # Write main consolidation data\n",
    "    df_excel.to_excel(writer, sheet_name='Consolidation_Report', index=False)\n",
    "    \n",
    "    # Get workbook and worksheet\n",
    "    workbook = writer.book\n",
    "    worksheet = writer.sheets['Consolidation_Report']\n",
    "    \n",
    "    # Add formatting\n",
    "    header_format = workbook.add_format({\n",
    "        'bold': True,\n",
    "        'text_wrap': True,\n",
    "        'valign': 'top',\n",
    "        'fg_color': '#D7E4BC',\n",
    "        'border': 1\n",
    "    })\n",
    "    \n",
    "    number_format = workbook.add_format({'num_format': '#,##0'})\n",
    "    \n",
    "    # Format headers\n",
    "    for col_num, value in enumerate(df_excel.columns.values):\n",
    "        worksheet.write(0, col_num, value, header_format)\n",
    "        \n",
    "    # Auto-adjust column widths\n",
    "    for i, col in enumerate(df_excel.columns):\n",
    "        column_len = max(df_excel[col].astype(str).str.len().max(), len(col) + 2)\n",
    "        worksheet.set_column(i, i, min(column_len, 50))\n",
    "    \n",
    "    # Format number columns\n",
    "    qty_col = df_excel.columns.get_loc('Sugested Qty transfer')\n",
    "    soh_cols = [df_excel.columns.get_loc('From Store Current SOH'), \n",
    "                df_excel.columns.get_loc('To store Current SOH')]\n",
    "    sales_cols = [df_excel.columns.get_loc('From STORE sales'), \n",
    "                  df_excel.columns.get_loc('TO store Sales')]\n",
    "    \n",
    "    for col in [qty_col] + soh_cols + sales_cols:\n",
    "        worksheet.set_column(col, col, 15, number_format)\n",
    "    \n",
    "    # Add summary sheet\n",
    "    summary_data = {\n",
    "        'Metric': [\n",
    "            'Total Movements',\n",
    "            'Total Quantity to Transfer',\n",
    "            'Products Involved',\n",
    "            'Source Locations',\n",
    "            'Destination Locations',\n",
    "            'Average Transfer per Movement'\n",
    "        ],\n",
    "        'Value': [\n",
    "            len(df_excel),\n",
    "            df_excel['Sugested Qty transfer'].sum(),\n",
    "            df_excel['Part No'].nunique(),\n",
    "            df_excel['From Store'].nunique(),\n",
    "            df_excel['To Store'].nunique(),\n",
    "            df_excel['Sugested Qty transfer'].mean()\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_df.to_excel(writer, sheet_name='Summary', index=False)\n",
    "    \n",
    "    # Format summary sheet\n",
    "    summary_worksheet = writer.sheets['Summary']\n",
    "    summary_worksheet.set_column(0, 0, 30)\n",
    "    summary_worksheet.set_column(1, 1, 20)\n",
    "\n",
    "print(f\"Excel report created successfully: {excel_path}\")\n",
    "print(f\"File contains {len(df_excel)} movements across {len(df_excel.columns)} columns\")\n",
    "print(\"Sheets created: 'Consolidation_Report' and 'Summary'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
